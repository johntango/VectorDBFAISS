Hi, so let's talk about the first steps of spinning up a data team in the cloud. So we're certainly going to need a lead data architect. We're going to need data scientists, and we're going to need people that understand developing software in the cloud. And most important, we're going to need domain experts that know the business side of your operations. Now we're going to need to choose a cloud, and we're going to go to the cloud because ultimately to get scale these days, you really need to operate in the cloud. We're probably going to need to choose a project that lasts maybe three or four months that can be our first win, that can show us how the tooling that we're adopting works, and what are the data sources that we can leverage. We're also going to need to set up internal communications, either using Slack or MS Teams, to record how we're learning and to be able to communicate when things are not going right. We need telemetry, for example, into the cloud of our product. So these are the kinds of things that we need to build. Ultimately, we're going to have to learn fast. And one of the things that distinguishes the best companies today are the speed at which they learn. And to do that, we're going to leverage Google, YouTube, great sources of information. I spend most of my time learning from YouTube. We're also going to leverage GitHub and NPMJS, the node package management system that comes with over a million libraries. There are 60 billion downloads in a month. So these are tremendous sources of information. Now we're not going to be the first people to do this. There's a great example of Nordstrom moving to the cloud. They had a project called Hello Retail, where they developed a sample application in the cloud. They leveraged the software as a service capabilities of the cloud. So for example, they used Kinesis of AWS to manage all their messaging. They used DynamoDB for their databases. And you'll see that there's lots of Lambda functions around. These are serverless, highly scalable functions that basically we don't have to manage. That Amazon will take care of the scaling. They'll automatically add machines. And these services are highly scalable. Basically the cloud provider takes care of the scaling. Now I want you to learn by imitation. It's a great way to learn. I'd say that all of us learn the most when we imitate experts. I've already run these examples for you in the cloud. We're going to do an example of handling big data using Spark. And we're also going to do some machine learning using Amazon's SageMaker. So the first exercise, you need to get a login and password on AWS. You'll need to back it with a credit card, but I promise you, you won't spend over $10. Next thing is you need to download some files. And to do that, we're going to ask you to use your terminal window. Terminal window, your bash window is where you can really control your machine. You can find files quickly. You can create directories, but you can also run commands like the command I'm going to ask you to use. To download the largest file, you need to use cURL. And cURL is an app which launches an HTTP request to a remote server and you can download large files. So here we're going to download this book data file, which is 140 megabytes. And the command is cURL minus O bookdata.json. That's the file's name on your machine. And then the URL of the server that you're going to hit to download it. The other smaller files, the Python files, you can just download in your browser. Just enter the URL into your browser and they'll automatically download. You'll need to be able to save them to cut and paste if you like from your browser, but that's easy. Just save them. You're later going to push these up to Amazon, to AWS. We're going to run Elastic Map Reduce and that's Amazon's big data platform if you like. We're going to spin up a cluster and use Spark that allows you to handle queries on distributed data. So you won't have to think about which data is where on what machine that Spark will handle this for you. So if you're handling big data, you're probably going to be using Spark. And we're going to control this from a Jupyter notebook. Hopefully you've heard of Jupyter notebooks. If you've done any Python, I'm sure you have, but we're going to use these to control these remote machines. And it's a beautiful way of controlling things in the cloud. And the second example is Amazon SageMaker, where again, we're going to use a Jupyter notebook to do some machine learning. We're going to do some reinforcement learning on a very, one of the standard problems called cart and pole, where we need to teach a machine how to balance. It's like balancing a ruler, if you like, on your hand. We're going to figure out what do we need to do to the cart, that's our hand, to balance the pole. So here's some books that I think you'll find interesting to read. First about the revolution that went on in manufacturing production. So it started with Toyota production system, then with lean engineering. And those principles have now been moved to the software world where we have lean DevOps, for example. And these are the books I'd recommend you to read, Accelerate by Nicole Forsgren, Jean Kim and Jez Humble, and Powerful by Paddy McGord, that details the Netflix journey where in 2010, they realized that shipping DVDs was not the future and that they needed to move to streaming video. And also part of the problem with software is we need to make it visible somehow to see where we are in our projects. And there's a great book on how we can do that. And then finally, the book Project to Product by Mick Kirsten, which deals with value chains. So I hope you enjoy these exercises. I hope you enjoy touching the cloud. So bye for now.