So our goal is to understand the modern data stack. But it turns out that there's not just one stack. That depending on our applications, we'll have variations. But there are some things in common. So let's step through the data ingestion pipeline. So we're going to start with raw data feeds. So these could be coming from our online website. Apps. Imagine that you're Uber or Lyft. So your app is producing data about the customer's needs. And if you're in operations, you've got sensors that are producing data. For example, if you're a chemical plant, you need to control it. And so you'll be monitoring your sensors on various machines and pipelines. And also we'll have third-party feeds that, for example, Salesforce might be feeding us data about our sales, etc. And so we've got a lot of these raw data feeds. Now with each one, we're going to have to go through steps. Here I'm showing one of the steps is that if we've got online apps or a website, then we've probably got Google Analytics that is giving us information about what the user did on our website, where they clicked, etc. So let's assume that we've got Google Analytics that is updating every minute or two minutes or five minutes, whatever we want on what's happening on our website. Now we need to extract that data from Google Analytics. And 5tran has a lot of connectors. In fact, most of the clouds now have lots of these connectors, which will connect you to things like Twitter, Facebook, here Google Analytics. So now this will allow us to extract from Google Analytics the data, and we can pipe it into some data store. Here I'm showing Snowflake. There are many other data stores that we could pump it into. But now we've got control of the raw data, usually in the cloud. Now we used to do what we called extract, transform, and load. But today we do extract, load, and transform. Because of the cloud capabilities, we can just load the raw data into the cloud, and that's what I'm showing here. Now we're going to process that raw data. We're going to clean it, and we're going to perhaps enrich it. And to do that, we need to make sure we keep track of what we did to the data. Now DBT is a great tool for that, and it's highly popular. And DBT allows us to specify queries on the data, that we can clean the data, we can enhance it. And also, whatever we do, we can record in GitHub. So we've got a record now of what we're doing to our data. So this is rather like the DevOps. Here we've got DataOps, where we're keeping track of exactly what we've done to that data. Now we're going to allow querying of that data store. And it could be complex querying if we're using this for business intelligence, which I'm showing here. And I'm showing some of the tools that you'd use for business intelligence. There's Google Data Studio, Tableau, Microsoft's Power BI, Looker, and there's a number of others that I'm not showing here. But that would be a typical pipeline that we would automate and put in place. Now there's several things missing from here. Here I'm just showing that we could have Redshift instead of Snowflake, BigQuery would fit well there, Microsoft Synapse and Databricks. Databricks and Snowflake exist on all the clouds. So we've got mainly we've got the three cloud providers. So we've got Google, Microsoft and AWS, Amazon. And then we have Databricks and Snowflakes. So those are the five big ones that you'll probably have in your pipeline. Now we'll come back a little later and take a look at a more complete architecture. We'll look at Uber and how they built their data pipelines. But for now, that's our modern business intelligence pipeline that provides analytics and BI capabilities so we can build dashboards, we can build reports on our data and make decisions. So we'll come back and look in more detail at how this gets enhanced when we, for example, want to, let's suppose, we're Uber and we want to do surge pricing where we're feeding back in real time to the user the cost of their trips. Okay, bye for now. Thank you.