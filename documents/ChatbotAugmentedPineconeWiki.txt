Hi, let's take a look at one of the problems with large language models. They're trained on large amounts of data and that means they can't be updated every second of the day. So their training data is going to be out of date. In some sense, their knowledge is frozen in the past. Now if we want to use them in, for example, a chatbot, then we need some way of bringing them up to date. So let's see how we might do that. We can store present knowledge in an online database. Here I'm showing a vector database that Pinecone runs for us. And it's a special kind of database because we've seen that we can, for example, embed text and whole phrases of text or whole sentences in an embedded space. And if we do that, we can reason about that text. And so what we're going to do is we're going to take the user's question and we're going to embed it in the same space that we've embedded recent knowledge. We're going to take Wikipedia and embed it in a vector space and we're going to use that then to update our queries to the chatbot. So what I'm showing here is we may have the user question "Who was M?" and we're going to embed that. So we're going to get a vector here, this little red vector. And then we're going to compare it with our knowledge that's up to date. And here I'm showing that we've got three chunks that are relevant. So we're going to do a vector.product probably in this vector space, in our embedded space. And we've found three documents that are relevant to "Who was M?" and we're going to return those and we're going to embed them in the prompt that we sent to our chatgpt chatbot. So that's how we're going to solve it. Just to give you an idea of how big our prompt we can give to these models. For example if we're using GPT-4 we can use about 10 pages of text. So that's quite a number of Wikipedia queries that we can return. OK, so let's take a look now at a running example of doing this.