Hi, so I want to quickly go over the boosting algorithm based on the lecture of Professor Patrick Winston at MIT. And let's take a look at how this is going to work. So let's suppose we have these points, one at 0, 0, one at 2, 2, another one at 4, 2, another one at 4, 4. Now let's suppose there's some property associated with those points. I'm going to have the red ones be true and the green false. So there may be some other property, but for now let's do that. And we want to build classifiers, a number of weak classifiers, that if we use them all, we'll actually classify all the points. So here we could see, for example, that we might be able to use two classifiers. So the first one would be, for example, that if x is greater than 3, it'll be true. So that'll get these two red points correct and it'll get the green point correct. But it'll misclassify the one at 0, 0, this other red one, because everything to the left of this classifier says, no, it's false. Everything to the right is true. But if we add a second classifier, that x is less than 1, that it'll be true, then it'll get this point correct, it'll get this point correct, but it'll get these two wrong. So somehow we have to weight these classifiers so that even though this classifier gets these wrong, it's weighted less than this original classifier. And finally, we'll see that if we just have these two classifiers, we'll actually misclassify a point such as this at this point, say at 0.52 in the y direction. And to get that, we need to add a third classifier. That if x is greater than minus 1, everything is true. OK, so in that case, this would be true. This would be true. It would get this wrong. But the weighting of this would be overwhelmed by the others. So that's the kind of idea in these classifiers. And I've written a little program and we can see it run as to how this classification works. And I've got a print out here of the final results. And just at the bottom here, let me just highlight those so you can see them. We're classifying points such as 0.5, 0.5, and that one should be true. Classify 2, 4. That should be false. So if we look back to where 2, 4 is, 2, 4 would be down here at the bottom in this area where I'm pointing. And that should be false. So let me explain roughly how these classifiers work. So let's suppose we have this first classifier, x is greater than three. So it's going to get the point 0.42 correct. It's going to get the point 0.44 correct, because both of those have x greater than three. It's also going to get the point 0.22 correct, because x is less than three. And so that will be false. But it's going to misclassify the point 0, 0. So if we have any red points to the left of this line, they're going to be misclassified. Now, the really good thing that this algorithm does is it says, OK, so this classifier works for these three points. What I'm going to do is I'm going to unweight these and not count them so much because they're already classified. So for the next classifier, this point 0, 0 is going to have a much larger weight because we want to make sure that the next classifier gets this right. So in fact, if we look at the results, we'll see that initially the weights of all these points are equal. So they're all 0.25, 0.25. Now what happens is, and let me, I'm going to have to squish this over a little bit so you can see it. But that on the second round now, here, we've got a weight of that first point. The 0, 0 point is weighted 0.5. So all the others are 0.16. So all the correct ones on that first round have their weights, so they sum to a half. So these 0.16's all summed give you 0.5. And then all the incorrect ones are weighted so they sum to 0.5. So in fact, there's only one that's incorrect and so it sums to 0.5. And now we apply the next classifier and that classifier now reweights. It's going to get everything correct. Now the two classifiers combined get everything correct. So their weights now of all the points sum to 0.5. So their weights are 0.125 and 8. So we have four of them now. And so that's kind of how this works. These classifiers are specialists. They get some points right and then others not right. And the ones that they don't get right are weighted heavily so that the next classifier will reclassify them correctly. So that's basically how it works. So take a look at the code. Here's the code here. I've changed the online one slightly. There was an error that I found in that. But take a look at the code and you'll see that exactly what we were talking about going on. That this weighting here, for example, of the known ones, that's the initialization. But then we reweight them here, set the weights. And you'll see that depending how many we have, we're always going to sum them to a half. The correct and the incorrect. OK, so take a look at the code, learn how to step through it, and we'll discuss it in class.