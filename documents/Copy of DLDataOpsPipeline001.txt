So let's talk about the modern DataOps pipeline. We've mentioned the DevOps pipeline, which revolutionized software development. And now we're seeing this DataOps pipeline also evolve. So here we have the original data sources. This could be operational data from supply chains. This could be customer resource management data, website data, for example, from Google Analytics or accounting data. We'll call this the raw data. Now, we need to get control of that data and get it into a place where we can analyze it and take advantage of what the data means. So we first need to have what we call connectors. 5tran, Stitch, Google, they all provide connectors these days. 5tran has probably the most, but all of the clouds have a considerable number of these connectors. Now, what that allows us to do is to plug into, for example, Salesforce so that we can extract the data from it. So this is the extract. And it used to be we'd extract and transform it, but these days we'll load it straight into one of the clouds. And so here I'm showing BigQuery, one of the Google clouds. But it could be Redshift in Amazon or Snowflake or Databricks. So we now have it in the cloud where we can push considerable compute resources at it to do the transformations that we need to do. Now, the transformations are taking that raw data and changing it. And we need to make sure that we record those transformations so that we can undo them or see what transformations we used at any particular stage. So DBT allows us to specify basically SQL transformations that we do on the data. And we can record those into GitHub. So now we have a good way of tracing exactly what's happened to our data. Now we're going to put that transformed data back into our data warehouse or lake house if it's Databricks or Redshift if it's AWS. So we're going to put it back. There's no need that it be in the same cloud. But here I'm showing that we're putting it back into Google's BigQuery. Now that we've got the source truth, if you like, of the data, we'll use a tool such as Looker to record exactly what that data is. Often when we use DBT, we'll be changing the names, for example, of columns so that they reflect our business purpose rather than perhaps sales forces. And we'll record those now in Looker. And Looker will become our source of truth for doing our business intelligence. And here I'm showing that we're using Google's Data Studio to do the business intelligence. But you could use Tableau or any of the other tools that are out there. And in some of these areas, there's a considerable number of tools. I mentioned the data warehouses, lake houses, Redshift, BigQuery, Snowflake, Databricks. There are a few others. But those are the main ones. Here we're just showing now that once we have control of it, we can enrich that data. We can use that data to create machine learning models. And we can then do reporting on our dashboards, predictive analysis, and feedback in what we call reverse ETL into our CRM system, for example. So that's basically where we are today with our DataOps pipelines. There's a lot of other tooling out there. And this is a diagram that Andreessen and Horowitz have put out of typical pipeline tools. And you'll see them classified slightly differently. But it's showing the movement, if you like, from here ETL to ELT, which I described, and the tools for data flow management, for self-serve insights, and for global data governance. OK, so that's the modern DataOps pipeline. Bye for now.