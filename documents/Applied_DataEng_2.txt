So in this section, as a data engineer, you're going to need at least one programming language. You'll probably need more than that, and certainly you'll need some scripting languages as well. We'll review briefly Python and Pandas, and why Pandas data frames are so powerful. We'll be leveraging Dask so that we can execute things in parallel. Again, today, data is about operating at scale, and we need to do parallel operations. Dask can help us. It's specifically targeted at Python, and it's very easy to use. We'll be showing you how to read and write tens of files at once, and we'll be exploring in more depth the DevOps pipeline and how we can leverage GitHub Actions to automate the continuous integration and continuous deployment. Usually it's applied to code, but we want to apply it to data, these data pipelines that we're setting up. We'll be talking about how data formats, how you store data on Dask, for example, so you can store it as a comma-separated variable file, but if you store it as, say, a Parquet file, it's much more efficient, and it's actually revolutionized the way that companies such as Databricks and Snowflake handle very large data. We'll be doing a case study based on Uber and their surge pricing, where they're taking information from your smartphone, you're telling them where you want to go and when you want to go, and in seconds they're replying with how much it'll cost. Now, if you're, for example, in London and you've just been to a show at night, there'll be lots of other people competing for those Ubers, and Uber does surge pricing, and you need to understand how can they quickly, how can they, so you need to understand how can they do these calculations so quickly, how do they handle the data that flows from your smartphone to their data stores, to their machine learning, and then they're issuing back to you the solution that they've come to, the optimal solution for your surge pricing. We'll be looking at streaming data, it's a subset of what we've been talking about with Uber, and finally we'll be looking at data governance. It's becoming a real challenge. In Europe we already have GDPR, and in the States we have quite a few different standards that we need to follow, and so data governance is becoming critically important to most companies. So this is the advanced stage of data engineering, and we're touching some of the tools that you'll need to execute your data pipelines. Bye for now.