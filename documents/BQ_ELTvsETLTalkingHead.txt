So there's a debate going on. Should we be doing ETL or should we be doing ELT? And what do those letters stand for? Well, it's extract, transform and load, or extract, load and transform. And what are we talking about? Well, we're talking about the data pipeline. That ELT or ETL is a really important part. It's where the data gets pulled into a database and transformed so that we can make predictions from it and do business intelligence on it. About eighty percent of the time is spent in doing ETL or ELT compared with machine learning and the data analytics that we do for business intelligence. And it's really important to get clean and well structured data. So let me just walk you through quickly this pipeline. We've got raw data that's coming from perhaps a website, Google Analytics for example, that's monitoring what's happening on your website, or it's coming from the factory where the machines are telling you how they're performing, or perhaps it's coming from a smartphone where you're using Uber to call for a taxi. But this raw data needs to be pulled into a database eventually, where we can then manipulate it and query it and make decisions about it. So the extract bit is just getting it from the raw data source. Now usually in the past we'd pull it into one of our servers and we'd actually start cleaning it. We transform it on our laptop, say, by querying our own local data store. But today we tend to get it straight into the data store. So we extract it, pull it, the raw data into the data store, and then we'll start transforming it, cleaning it, etc. Now the reason that we're doing that is because today's data stores, data lakes, are based on blob storage or S3 type storage, where we can just store objects and it's very cheap. And we'll go into more detail about why Databricks and Snowflake are so performant in that they've separated compute from storage. So because it's in blob storage or in S3 storage, it's very cheap to store it. So you can store massive amounts of data and as long as you're not using them and querying them in a data store, so we'll be extracting them from S3 buckets, for example, and pulling them into our database and it'll usually be a column database that we can operate on those features, those columns, very quickly. And then we can get business intelligence from that data. Now the stage where you're doing the transformation is quite important and we've got tools today like DBT that allow us to structure the queries, structure the cleaning of the data, and actually track what we've done to the data. So DBT will create files that you'll then store in GitHub so that you can track them. But there are of course other ways. When we used to do the transformation before the loading, we'd be doing it on our laptop and we had tooling such as Python Pandas. And it turns out that Python Pandas is actually very performant. I've loaded a million line file into a Pandas data frame and it didn't take that long. So we could do ETL quite easily and it only becomes critical when the files get quite massive. So if we have petabyte files for example with hundreds of millions of lines then it becomes a real issue for us. So it's only when we get petabyte files that we really can't handle on Python data frame. And the reason we can't handle it is because we don't either have enough memory or that it's not fast enough. And Pandas is limited to a single machine and a single process. Whereas if we bring it in to something like Databricks where we have Spark, we can distribute that petabyte across a whole cluster of machines. So thousands of machines where we've chopped the data up into pieces and we can handle those petabytes. And Snowflake does the same. So we've got the blob storage or the object storage as it's called or the data lakes are usually on S3 buckets or on Azure lakes. And now we can get it into memory and make the analytic queries that we need to make. So if we're pushing it now to, for example, into Tableau, that Tableau will be making the queries on the lake house or the structured data in Databricks for example, so that it can answer those queries really quickly. So we're going to have lots of examples of handling data and we'll be seeing that ELT is extremely important. That today we are operating in the cloud that gives us the ability to handle these massive files. So that's the story of why today it's more like ELT than ETL. Okay, bye for now. [door opens]