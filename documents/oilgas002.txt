OK, so let's quickly go over ordinary linear regression. So our basic equation is that we have data points that we are randomly choosing from a distribution, a single distribution that has a single mean and a single variance. So we have our independent variables, x, and we have y equals x times beta, where beta are our model parameters. So if we're doing, say, a straight line like linear regression in xy space, this would be, the betas would be the intercept and the slope of the line. So the betas are the things we're trying to find and in general we won't be able to put the line through all the points, so we're going to have some error term e. So y equals xb plus e. Now let's write those out. So now we have our y1s, y2 up to yn equals, and on the right hand side our x's, the intercept is going to be beta zero, so we have a column of ones that multiply that because it's the same intercept. We're going to have a single intercept and now our x1, x2, x3 are going to be multiplying the beta one, the slope. So what do we know? Well we know that our predictions here using our model can only lie in the column space. So the column space is this vector of ones and the other vector is the x1, x2, x3 vector. So we have these two vectors. Now we can only predict points in that column space and we have an error because our solution doesn't lie in that column space. And what do we know about the error? Well it's perpendicular to the column space. So what this means is that x transpose, if I take the transpose of these x's and I dot that with e which is perpendicular to each of those vectors, then it's got to be zero. So we can solve this algebraically by multiplying through by x transpose. So we get x transpose of x beta equals x transpose of y and then the error term has disappeared. Now we can take the inverse of x transpose x and we get beta equals x transpose x inverse to the minus one times x transpose y. That's our solution for the betas. Now we've made some assumptions. We've made the assumption that all of our independent variables are drawn from identically the same distribution. So what we mean by that is a single underlying model distribution. Say a normal distribution with a single mean and a single variance. Now that's not always true. And so what do we do? Well you've probably heard of the variance-covariance matrix. So our variances, the sigma squareds, are down the diagonal. So we're saying now, and if it was a single variance for all of these variables, that that sigma, there'd just be one sigma squared. It would be every term on the diagonal would be the same sigma squared. But if the variance varies, then we'll have different sigma squareds down the diagonal. We'll have sigma squared x naught variable, sigma squared for x one, sigma squared for x two, etc., down to sigma squared of x n. Now if our data is also correlated in space, say spatial correlation, we will have terms on the off-diagonal, we'll have, here I've written them as w x naught x one, w x naught x two. So these are kind of weights, how much a variable or a reading, a data point at one position x naught affects data at another position x one. So these we'd have to determine somehow. But for now, let's assume we have this matrix, let's call it omega. We can rewrite it, we'll abbreviate it here, where I've got omega equals sigma naught squared, sigma w naught one, w naught two, etc. Now the thing to notice is that this is a symmetric matrix. Even though we have all this coupling, it's symmetric and it's positive definite. Now, since it's symmetric and positive definite, we can do Cholesky decomposition on it. So what I mean by that is we can rewrite omega as p p transpose. We know we can do that. What does this mean? Well, let's see. Let's take an example here. Let's work through this example. I've taken omega equals, I'm going to assume just two variables, and I'm saying it's a two by two matrix now, and let's have ten and one on the diagonals. So this means the sigma zero squared is ten and the sigma one squared is one. So what are we doing there? We've got different variances, somehow we've scaled things. On the off diagonal I've got minus three and three. And I write my p and p transpose, and p is the square root of ten and zero, so it's lower triangle, and we've got minus 0.95 and 0.31. And p transpose is obviously upper triangle and just transpose those rows and columns. Okay, so let's do an experiment. Suppose I take my x's and let me scale them by a factor p. Well, if p is greater than one, it means that those x's will get further apart. We'll be multiplying each x, say, by two or four or ten, and that means they're going to be stretched. And what it means is that the variance also is going to change. So let's take some uncorrelated random variables, let me call them z. And that means, let's take them to have a mean of zero and a variance of one. So we'll just draw them from that distribution. And let's transform them so that now we're going to multiply the z's by this factor p. What's going to happen is that the result is going to be a set of correlated values. So let's write x equals a plus pz. a is just a constant, but that's going to change the mean. The p is going to change the variance. So let's plot those out. And we're doing it for this matrix p that we've just calculated from the omega. And we'll see that these give correlated values. Now, how could we uncorrelate them? Well, we multiplied them by p to make them correlated. So if we just pre-multiply by p to the minus one, so p to the minus one p of z will take us back to z. So we can create correlated variables, but suppose we are given correlated variables, this means we have a means of uncorrelating them. And we're going to see next how to do that. Let's now multiply our correlated data by p to the minus one. And as you see in the result in our Jupyter notebook, it produces random, randomly distributed variables again. So we're going to use this trick, if you like, to uncorrelate correlated data when we have it. So we need to find the omega matrix, do Cholesky decomposition on it to find our matrix p, and then we can calculate p to the minus one. And this is what generalized linear regression does. So let's take a look now at generalized linear regression.