Hi, so I want to take you through the boosting algorithm and we're going to step by step go through one particular case. So boosting allows you to use multiple weak classifiers to obtain a strong classification. So here we're going to have four points, four data points that are going to be our base data and then we're going to classify other points using these classifiers. So we have a data point 0,0 and that happens to be true. I'm marking the data red if it's true, green if it's false. So the point 2,2 is false, 4,2 is true and 4,4 is also true. So we can segment this data and we might use a classifier for example that if x is greater than 3 it's true. Now that classifier would classify this point 4,2 correctly and the point 4,4 correctly and the point 2,2 which then is false because it lies to the left, it's less than 3. And it would misclassify 0,0 and so we might have another classifier that x is less than 1 and that would classify correctly 0,0 and also 2,2 but misclassify the other two. Now the thing about boosting is that it gives weights both to the classifiers and as we move through the algorithm to the data points. So the idea is if I have a classifier that's very good on certain data points, for example this first classifier x is greater than 3 classifies all three of these points to the right here correctly. And so I've got those correctly classified, it's just this one I need to worry about. So what it will do is we'll now re-weight the data points so that this point 0,0 counts more than these others. Let's take a look at how that works and we note that we need to have, in this case we're going to need three classifiers so we can classify new data for example this point at 0,0,0,0,0,0 would be misclassified if I just had two classifiers. So here's some of the mathematics. So we're going to develop a classifier that has weak classifiers H1, H2, H3, etc. Each one of those is going to be weighted by alpha. We're going to choose the alpha weight of each classifier depending on the error that it produces. So for example the error of H1 in the first step is going to be, well we got three right and one wrong. At the first step all the data points count the same, they're all weighted at 0.25. So the error is 0.25, a quarter. We get one out of four wrong. So we've got a quarter and on the top one minus a quarter so that's three quarters. So we get log of three and a half of that and it turns out that's 0.54. So alpha in this case would just depend on that error. Okay, so it depends on the number of misclassified data times the weights of those data at that time. Okay, and we weight now the misclassified data and the classified data according to this. That depending what the weight was at a certain step t, we're going to calculate it at step t plus one according to this. Where we've got the alpha in there and Ht is our classifier and y is either plus one or minus one depending on for example whether it's true or false. But it'll always have this as a, leave this as a negative quantity overall. So for example, once you work through the mathematics, you can show that the way to weight the incorrect data is the sum of the weights of the incorrect data should all be the same and should sum to a half. And then the correct data should also sum to a half. So in the case that I've got three correct data points, it means that it's a half divided by three, that each one of these is going to be weighted at 0.16666. And this is going to be weighted at, there's only one incorrect at the end of the first step, and that would be weighted then at 0.5. Okay, so we'll see how this works. So here we are at the first step. All the points are weighted equally with a sum that they sum to one, they're a distribution. And now we choose our first classifier. And so x greater than three is true. So that gives this point for two is true. This is true. This is true. This one we misclassify. So we get three out of four correct. The alpha will be 0.54. This classifier, x less than one, okay, is going to get two wrong. So it gets two right and two wrong. It turns out that the alpha for that then is zero. Okay, because we get the number of, when we go back to the equation here, we've got one minus the error. So we've got one minus 0.5 over 0.5. So that's the log of one, and that's zero. So alpha will be zero. This classifier is not going to, in the first round, is not going to take part. It's got an alpha of zero. Now we look at this one, and this one is a good classifier as well. It gets three out of four correct. It gets this one is true, this one is true, this one is true. It gets this one wrong, the two-two point. So again, this one will have, this classifier will have an alpha of 0.54. And so this is the situation at the end of step one. We've got alpha one with this weight, alpha two with this weight, sorry, classify two with this weight, and classify three with the same as one. And because one was there first, we're going to choose one as the first classifier. Okay, so H1 is classifier one, and now we move to step two. And we have to update the weights now of all our data based on the fact that we've got this one classifier. We've got these other two now waiting in the wings. So the first classifier gets three out of four correct. And so the weightings now of the points are these. These three points on the right here get a weighting of 0.16, and the misclassified one gets a weighting of 0.5. Now with those weightings, we reapply the other two classifiers. So we apply this classifier. Now in this case, we get two wrong. Okay, so we get these four, two and four, four wrong, but their weights now are only 0.16. So the total error now is 0.33. We plug that in to here and we get the log of two and a half of that. And it's 0.346. So now this one, which had an alpha before of zero, gets an alpha of 0.346. Now we've got to deal with the other classifier. So this is the one x is greater than minus one. Now this classifier gets that one correct and that correct and that. It only misclassifies this one, the green one. So now the error is 0.166. And when we take the log of this number, it's 0.8. So alpha now of this classifier is 0.8. So on the second round now, we've got our second classifier specified, H2, and it's going to be a classifier three. Okay, so now we've got H1 and H2 with their alphas. H1 is an alpha of 0.54. Our second classifier is an alpha of 0.804. Now we move to step three and we update the weights again. Now because these two classifiers are classifying all the weights or all the points correctly, we've got four correct points now with these two classifiers. So the weighting is a half divided by four or one eighth, 0.125. So we've got this situation now and now we apply this final classifier here that x is less than one. This gets two incorrect. Right, but so it gets this correct and this correct, but it gets these wrong. But that only comes to 0.25. So this has an error of 0.25 and its alpha now will be 0.549. So at the end of step three, these are the weights of all the classifiers. Okay, and so classifier two is chosen as H3 and has an alpha of 0.549. Now we can apply the classifier to new data. So here I'm taking the point 0.5, 0.5, and it returns true, as it should. I classify the point 0.24 and that should be false and it is. And I classify the point 0.4, 0.5, 0.4, 0.5, and that should be true and it is. So our classifiers now, we've got a strong classifier where we first had a weak one. So let me show you, if I scroll up, you'll see that we classify, these are classifying the points. I need to move that up for you. There you go. And if we look at our debug, let me bring the console up here. Here, we can step through and see the weights changing. So here I've got three classifiers that I'm adding. I've got the data and you'll see the values of the data, the weights of the data point changing. So for example, here after step one, so this is step two, the weight of the point at 0,0 is 0.5, the weight of the other three 0.16. And as we step through, we see these weights changing again and finally we get our classifiers that our data now finally has weights all the same. They're all classified well and the good classifiers have alpha of 0.549 and an alpha of 0.804 and 0.549. So just as we had in the slides and we're classifying the points and they're getting classified correctly. Okay, so that should help you and we have the code that I can give you, but that's how boosting works. Very clever algorithm. Okay, cheers.