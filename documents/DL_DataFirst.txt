So, data leadership implies that we're going to take some kind of journey, that we've got a starting point, and that we're going to move forward towards some goal. Now, we need to understand this starting point. Where are we today? And that's not obvious. So what I want to do now is recap where I think we are today. So over the last 20 years, a number of new technologies and tools have evolved that we can leverage, especially in the area of data. And that includes cloud computing, machine learning, and AI, the Internet of Things, blockchain, and cybersecurity. And we've been involved in all of these areas. Abel and I were heavily involved in the Internet of Things, and we took a similar journey. At the beginning, we thought it was a technical problem. We were trying to get EPC codes read off RFID chips. But it turned out that wasn't the problem. So we have to be careful. At the beginning, we think it's usually a technical problem, but often it's not. Now, there are many methodologies that have evolved over the last 50 years. We've talked about some of them, the Toyota production system, Lean Engineering, DevOps, Lean DevOps, Agile. And we need to understand where to apply each one of these. They all have some merit. Now this diagram by Pam Mantri and John Thomas, I think emphasizes that point, that each of the methodologies has some area of application. So for example, waterfall works quite well if it's a known area that we understand the problem. There's lots of examples of how it's been solved. And so we can use waterfall quite well in that case. Now there are other areas where waterfall certainly doesn't work. That 20 years ago, we used to do software development and there was a three to five year cycle for major products. Microsoft took three years for Visual Studio. Now today we don't develop on that kind of clock tick because things in the external world are evolving very rapidly. In fact, it's estimated that knowledge is doubling every two years. So just the events of the last few years with COVID, price of oil going up to $120 a barrel or more, down to under zero for some small time, and then back up again today. Now we need to understand what kind of system we're involved with developing. I'm showing here a chaotic system that looks very simple. It's just two masses connected by two springs or two bars. And this is the path that the endpoint goes through. It's totally unpredictable that you'd need to know the exact starting conditions really precisely. And the problem with most systems that we're involved with developing is we don't know the initial conditions. We don't know a lot about how it's behaving. So we have to be careful about understanding what kind of system are we dealing with. Often it's very difficult to tell exactly what system. So for example, we're certainly building models of the whole earth for climate, but it's unclear are we capturing things properly or not. So one of the things that we have seen though is that today things are changing very rapidly. Now one of the good news points, or one of the points of great news is that in fact the global GDP is around 120 trillion. Now it may have fallen slightly due to conditions this year, but back in 2000, back in the year 2000, it was only 35 trillion. So it's doubled and nearly doubled again in that period of time. And this kind of curve that you see that's flat and then suddenly taking off, we see a lot these days. Nonlinear systems exhibit this kind of behavior where small inputs have big results on the output. We see this in our machine learning models, and it causes us great concern because it's difficult to predict is the model going to break at some point due to some small change. So we need to understand this world that we're living in, that things are changing rapidly. Here's an example of the cloud providers. Amazon is at the moment is the most dominant. Microsoft and Google are following up. But the clouds are slightly different. There's advantages and disadvantages to each of the clouds. I've operated in all three of them, and I can tell you that they're very different experiences. And it depends whether you're kind of coming from bottom up and just want to develop some simple pipeline or whether you really want to take advantage of having lots of pipelines and having machine learning. So we'll discuss that later, but this is our starting point. Now it's been said that APIs are the magic bullet, and it's true and it's not true. Jeff Bezos wrote a memo in 2002 that any software that was developed in Amazon needed to have APIs that could be published so that other groups could take advantage of them. Now an application program interface, an API, is just an endpoint, a URL that you can hit with a message. So typically we could send a message from our browser to hit an API and, for example, get a list of companies in the US. It would return that to us. Now the problem with APIs is if we have thousands of them and they're talking to each other, we have an n-squared problem that I'm talking to n of them, and the next person is also talking to n, and if one of them makes a change, everyone else has to adjust. And the problem when we're using these APIs is often the API itself doesn't really know who we are, that we just sign up and use it, and it doesn't know whether we stopped using it or not. It doesn't keep track of us like that. So APIs come with some conditions that we have to understand, and we'll talk more about them. But in general, the cloud has lots of apps, lots of facilities for us, components that will store data, will cache data, different kinds of data stores, column databases, row databases, SQL, non-SQL, etc., and they all have a role to play. So we take advantage of the cloud. Today we operate in containers, so we need to understand how containers work, and there's some issues there of orchestration, because the ports in the container map to physical ports, and often it's difficult to keep track of exactly what port is talking to what other port. But these cloud services in general give us a means of moving quite quickly and having stable pipelines that they will take care of patching, etc., of the software and of the components. And AWS, for example, has a whole ground station. If you launch a satellite, you can deploy a ground station, and you can pull the data from the satellite quite easily. Now what we're seeing these days is that small teams, what we call high-performance teams, can move quickly and have huge leverage. So for example, WhatsApp was developed by about 50 people and sold for 18 billion. So it doesn't take large teams, and these high-performance teams, we've seen with DevOps that they can move extremely quickly. Some of them are moving 400 times faster than the slower companies. Today some of them are moving, it's estimated, at maybe 2,000 times the speed. Now they can operate at speed, and because they have data, they can operate with less risk. We call these companies data-first companies. One of the examples that Abel and I like to quote is F1 racing. The F1 race car is monitored by a big team of engineers. They're monitoring everything in that engine. And as the car uses fuel and becomes lighter, they can adjust the structural performance of that car. They can change the damping, they can change the downforce from the wing, etc. Now it becomes extremely important sometimes to get it right. That for example, a car crashes, you have a couple of seconds to decide whether to pull your car in, to box it as they say, to change the tires or not, or to leave it out there. And that can make the difference between winning and losing. So having great data allows you to make very rapid decisions and either you win or you lose. Now, in farming we're seeing the networked farm where data is being fed to the farmer, and this is a typical tractor, and the script that's issued by companies like Monsanto tell the farmer what seed to plant in what part of the field, how deep to plant it, and they can predict what the crop will be. They have more data than the farmer and can farm that land better than the farmer can. So we're seeing these coordinated platforms. So we're seeing these coordination platforms like Uber and Airbnb, where they're having real-time price adjustments, where they're having surge charges, for example, with Uber, and very few companies can operate at speed and with the data to make those decisions. But that's what it means to be a data-first company. Clean data, good data, is critical for machine learning. So to some extent, data is driving new business models, is lessening the risk, and when we leverage machine learning, we can do things that the human can't possibly do. So these pipelines today, when we talk about APIs and apps, apps are talking to other apps. These are event-driven systems that we're building. Now, the architecture of the organization is changing. Jeff Bezos talks about the two pizza teams, and these teams can perform at scale. They can handle a pipeline. And so we're seeing continuous integration, continuous deployment, and the teams are quite small and agile. So if you're starting out, almost all the experts say, "Build one pipeline." Don't think about the architecture of hundreds of pipelines. Just start with one, get some experience, go to one cloud, master that, master the security, master all of the monitoring that you need to do, and then your skillset will be at a level, the next pipeline will be built much faster and much more reliable. So we're seeing in Smart City, for example, there's Dial 311 to report, for example, a pothole or graffiti. And in fact, an XMIT colleague of mine has that company that is operating both in Boston and San Francisco. And just this one app, so today it's not phoning, it's an app that you can report the pothole, you give the location, et cetera, or where the graffiti is, you take a photograph, send it in, they can decide which team to send out. So Smart City is starting down at that level of one app, which is giving tremendous leverage to the cities. So the advice is start small, move fast. They say break things, but I'm sure that'll just occur naturally. Make your progress visible, get metrics so that you can understand how you're performing. Now Microsoft with their testing, it took them two years to master testing at speed. And initially they actually went, they became worse. So initially they actually became worse. But today learning at speed is critical. So today learning at speed is critical. We're building minimum viable products so that we can learn quickly from them and improve them. So we built this course so you can take some shortcuts in learning. We believe that it's a great course and we've learned a tremendous amount from operating in these clouds and handling some decent sized data, petabytes for example. So bye for now. Hope you enjoy the course.